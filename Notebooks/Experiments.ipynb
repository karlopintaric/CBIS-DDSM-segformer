{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from albumentations import Compose, HorizontalFlip, VerticalFlip\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor, SegformerFeatureExtractor\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(images, random_state=0):\n",
    "    valid_images = []\n",
    "    \n",
    "    ptidx_unique = np.unique(np.array([str(x).split(\"/\")[-2] for x in images]))\n",
    "    np.random.seed(random_state)\n",
    "    valid_patients = np.random.choice(ptidx_unique, size=int(len(ptidx_unique)*0.20), replace=False)\n",
    "    train_patients = np.setdiff1d(ptidx_unique, valid_patients)\n",
    "    \n",
    "    for patient in valid_patients:\n",
    "        for path in images:\n",
    "            if patient in path:\n",
    "                valid_images.append(path)\n",
    "    \n",
    "    train_images = list(set(images).difference(set(valid_images)))\n",
    "\n",
    "    return train_images, valid_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBISDDSM_Dataset(Dataset):\n",
    "    def __init__(self, images, transforms=None):\n",
    "        self.images = images\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        mask_path = self.images[idx].replace(\"FULL\", \"MASK\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=np.array(image), mask=np.array(mask).astype(np.uint8))\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        # Use the feature extractor\n",
    "        feature_extractor = SegformerImageProcessor(do_reduce_labels=False, do_resize=False)\n",
    "        encoded_inputs = feature_extractor(images=image, segmentation_maps=mask, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "            encoded_inputs[k].squeeze_()\n",
    "\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBISDDSMDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, image_dir: str, batch_size: int = 8, transform=None, val_transform=None, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.val_transform = val_transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        images = glob(f\"{self.image_dir}/**/*_FULL.png\", recursive=True)\n",
    "        train_images, val_images = splitter(images, random_state=123)\n",
    "\n",
    "        self.train_dataset = CBISDDSM_Dataset(train_images, self.transform)\n",
    "        self.val_dataset = CBISDDSM_Dataset(val_images, self.val_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"../sample\"\n",
    "images = glob(f\"{IMAGE_DIR}/**/*_FULL.png\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = CBISDDSM_Dataset(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst[0][\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst[0][\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dst[0][\"pixel_values\"].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = CBISDDSMDataModule(IMAGE_DIR,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dst, 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))\n",
    "\n",
    "for k,v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"../data/train/processed\"\n",
    "TEST_DIR = \"../data/test/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = glob(f\"{TRAIN_DIR}/**/*_FULL.png\", recursive=True)\n",
    "train_images, val_images = splitter(train_images, random_state=123)\n",
    "test_images = glob(f\"{TEST_DIR}/**/*_FULL.png\", recursive=True)\n",
    "\n",
    "train_dataset = CBISDDSM_Dataset(train_images)\n",
    "val_dataset = CBISDDSM_Dataset(val_images)\n",
    "test_dataset = CBISDDSM_Dataset(test_images)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegformerFinetuner(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, id2label, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=100):\n",
    "        super(SegformerFinetuner, self).__init__()\n",
    "        self.id2label = id2label\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.train_dl = train_dataloader\n",
    "        self.val_dl = val_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        \n",
    "        self.num_classes = len(id2label.keys())\n",
    "        self.label2id = {v:k for k,v in self.id2label.items()}\n",
    "        \n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/mit-b0\", \n",
    "            return_dict=False, \n",
    "            num_labels=self.num_classes,\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id,\n",
    "        )\n",
    "        \n",
    "        self.train_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.val_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.test_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        \n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "        \n",
    "    def forward(self, images, masks):\n",
    "        outputs = self.model(pixel_values=images, labels=masks)\n",
    "        return(outputs)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        \n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        self.train_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        \n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        \n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        self.val_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.val_mean_iou.compute(\n",
    "              num_labels=self.num_classes, \n",
    "              ignore_index=0, \n",
    "              reduce_labels=False,\n",
    "          )\n",
    "        \n",
    "        val_mean_iou = metrics[\"mean_iou\"]\n",
    "        val_mean_accuracy = metrics[\"mean_accuracy\"]\n",
    "        \n",
    "        metrics = {\"val_mean_iou\":val_mean_iou, \"val_mean_accuracy\":val_mean_accuracy}\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        \n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        \n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        self.test_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "        self.test_step_outputs.append(outputs)\n",
    "            \n",
    "        return({'test_loss': loss})\n",
    "    \n",
    "    def on_test_epoch_end(self, outputs):\n",
    "        metrics = self.test_mean_iou.compute(\n",
    "              num_labels=self.num_classes, \n",
    "              ignore_index=0, \n",
    "              reduce_labels=False,\n",
    "          )\n",
    "       \n",
    "        avg_test_loss = torch.stack([x[\"test_loss\"] for x in self.test_step_outputs]).mean()\n",
    "        test_mean_iou = metrics[\"mean_iou\"]\n",
    "        test_mean_accuracy = metrics[\"mean_accuracy\"]\n",
    "\n",
    "        metrics = {\"test_loss\": avg_test_loss, \"test_mean_iou\":test_mean_iou, \"test_mean_accuracy\":test_mean_accuracy}\n",
    "        \n",
    "        for k,v in metrics.items():\n",
    "            self.log(k,v)\n",
    "        \n",
    "        self.test_step_outputs.clear()\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0:\"BACKGROUND\", 1:\"BENIGN\", 2:\"MALIGNANT\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.classifier.weight', 'decode_head.linear_fuse.weight', 'decode_head.batch_norm.bias', 'decode_head.classifier.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.2.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "segformer_finetuner = SegformerFinetuner(\n",
    "    id2label, \n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader, \n",
    "    test_dataloader=test_dataloader, \n",
    "    metrics_interval=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/nfs/home/912c2e22633dde2e/CBIS-DDSM-segformer/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type                             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model | SegformerForSemanticSegmentation | 3.7 M \n",
      "-----------------------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.860    Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/912c2e22633dde2e/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420caa1c155742c0b8ee276ec6d24f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/912c2e22633dde2e/CBIS-DDSM-segformer/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=0.00, \n",
    "    patience=3, \n",
    "    verbose=False, \n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1, \n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    max_epochs=500,\n",
    "    precision=\"16-mixed\",\n",
    "    val_check_interval=len(train_dataloader),\n",
    ")\n",
    "\n",
    "trainer.fit(segformer_finetuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5c72e5afc447ccbd87d04cee02ec6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">  Runningstage.validating  </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.30371415615081787    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val_mean_accuracy     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_mean_iou        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m Runningstage.validating \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.30371415615081787   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val_mean_accuracy    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_mean_iou       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss_epoch': 0.30371415615081787,\n",
       "  'val_mean_iou': 0.0,\n",
       "  'val_mean_accuracy': 0.0}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(segformer_finetuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "color_map = {\n",
    "    0:(0,0,0),\n",
    "    1:(0,0,255),\n",
    "    2:(255,0,0)\n",
    "}\n",
    "\n",
    "def prediction_to_vis(prediction):\n",
    "    vis_shape = prediction.shape + (3,)\n",
    "    vis = np.zeros(vis_shape)\n",
    "    for i,c in color_map.items():\n",
    "        vis[prediction == i] = color_map[i]\n",
    "    return Image.fromarray(vis.astype(np.uint8))\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    images, masks = batch['pixel_values'], batch['labels']\n",
    "    outputs = segformer_finetuner.model(images, masks)\n",
    "        \n",
    "    loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits, \n",
    "        size=masks.shape[-2:], \n",
    "        mode=\"bilinear\", \n",
    "        align_corners=False\n",
    "    )\n",
    "\n",
    "    predicted = upsampled_logits.argmax(dim=1).cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "    break\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "f, axarr = plt.subplots(predicted.shape[0],2)\n",
    "for i in range(predicted.shape[0]):\n",
    "    axarr[i,0].imshow(prediction_to_vis(predicted[i,:,:]))\n",
    "    axarr[i,1].imshow(prediction_to_vis(masks[i,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
